{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9dd3a8-c5a3-4612-9008-6e859ef895ad",
   "metadata": {},
   "source": [
    "# Real World Application\n",
    "\n",
    "Integrating our trained LSTM model, we envision a GUI for intuitive drone control via eye movements and blinks. This interface features buttons for commands like Main, Forward, Back, Lift, Land, Right, and Left. It continuously monitors eye gaze, using the LSTM model to interpret eye movements in real time. If a gaze fixes on a button for 3 seconds, the system deems it an intent to select that command. A subsequent blink within a brief window confirms the selection, acting as a \"yes\" to execute the command. Upon confirmation, the drone receives and performs the action. We utilize denoised EOG data fed into our real-time optimized LSTM model, which runs in a dedicated thread. Every eye gaze fixation is reported to the user through the AsyncTTS class for asynchronous text-to-speech operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d772d8-e7dd-4ae5-a9d5-a7a8f1aebbbc",
   "metadata": {},
   "source": [
    "### 1. Adding required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3150546c-d214-4aa7-92cd-dc0e1ef49a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from collections import deque\n",
    "import time\n",
    "import glob\n",
    "import serial\n",
    "import queue \n",
    "from queue import Queue\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import pywt\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, TensorDataset\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from djitellopy import tello\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "713dc847-ddbe-4ee3-b949-09d3d6cb9ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set the device to CUDA if available for GPU acceleration; otherwise, use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9780df36-d0de-45b2-a576-7375ec4b1bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] tello.py - 129 - Tello instance was initialized. Host: '192.168.10.1'. Port: '8889'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Tello drone\n",
    "drone = tello.Tello()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7baeaa2d-9cd3-4fcd-926f-00490db47b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] tello.py - 438 - Send command: 'command'\n",
      "[INFO] tello.py - 462 - Response command: 'ok'\n"
     ]
    }
   ],
   "source": [
    "# Connect to the drone\n",
    "drone.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3120b5b1-4d01-4e64-aada-19f1b3b78d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Battery percentage: 94\n"
     ]
    }
   ],
   "source": [
    "# Initialize drone battery\n",
    "print(\"Battery percentage:\", drone.get_battery())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7daafa1-50e1-408e-b697-9b91c40c8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_serial_continuous():\n",
    "    # Define a pattern to extract horizontal and vertical values from the serial input\n",
    "    pattern = re.compile(r'H:(\\d+), V:(\\d+)')  # Regular expression to match the expected format\n",
    "\n",
    "    # Continuously read from the serial port until an exit condition is met\n",
    "    while not exit_event.is_set():\n",
    "        # Check if data is waiting in the serial buffer\n",
    "        if ser.in_waiting > 0:\n",
    "            try:\n",
    "                # Read a line from the serial port, decode it, and strip trailing newlines/spaces\n",
    "                line = ser.readline().decode('utf-8', errors='ignore').rstrip()\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Ignore lines that cause decoding errors\n",
    "\n",
    "            # Attempt to match the read line to the expected 'H:###, V:###' format\n",
    "            match = pattern.match(line)  # Try to match the line to the pattern\n",
    "            if match:\n",
    "                 # Extract the horizontal and vertical values from the matched pattern\n",
    "                hor_part, ver_part = match.groups()\n",
    "                \n",
    "                 # Convert the extracted string values to integers\n",
    "                hor_value_int = int(hor_part)\n",
    "                ver_value_int = int(ver_part)\n",
    "\n",
    "                # Ensure the extracted values are within a valid range \n",
    "                if 0 <= hor_value_int <= 1023 and 0 <= ver_value_int <= 1023:\n",
    "                    # Queue the valid data tuple for further processing\n",
    "                    raw_data_queue.put((hor_value_int, ver_value_int ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9271b9be-4015-48c9-8050-c262804c2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal(signal, wavelet='sym20', level=1):\n",
    "    \"\"\"\n",
    "    Denoise a signal using Discrete Wavelet Transform (DWT) and soft thresholding.\n",
    "    \n",
    "    Parameters:\n",
    "    - signal: The input signal (1D numpy array).\n",
    "    - wavelet: The type of wavelet to use (e.g., 'db4').\n",
    "    - level: The level of wavelet decomposition.\n",
    "    \n",
    "    Returns:\n",
    "    - The denoised signal as a 1D numpy array.\n",
    "    \"\"\"\n",
    "    # Decompose to get the wavelet coefficients\n",
    "    coeff = pywt.wavedec(signal, wavelet, mode='symmetric', level=level)\n",
    "    \n",
    "    # Calculate the threshold using the universal threshold method\n",
    "    sigma = np.median(np.abs(coeff[-1])) / 0.6745\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(signal)))\n",
    "    \n",
    "    # Apply soft thresholding to remove noise\n",
    "    coeff_thresh = [pywt.threshold(c, threshold, mode='soft') for c in coeff]\n",
    "    \n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    denoised_signal = pywt.waverec(coeff_thresh, wavelet, mode='symmetric')\n",
    "    \n",
    "    return denoised_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "766fa4c3-208d-4103-98e2-caed314dda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal_continuous(initial_samples = 50):\n",
    "    # Initialize lists to accumulate horizontal and vertical data\n",
    "    accumulated_hor_data = []\n",
    "    accumulated_ver_data = []\n",
    "\n",
    "    # Continuously process incoming raw data until a stop condition\n",
    "    while not exit_event.is_set():\n",
    "        # Retrieve raw data, normalize it, and accumulate\n",
    "        hor, ver = raw_data_queue.get() \n",
    "        hor = (hor - 512) / 512 # Normalize horizontal data\n",
    "        ver = (ver - 512) / 512 # Normalize vertical data\n",
    "        accumulated_hor_data.append(hor)\n",
    "        accumulated_ver_data.append(ver)\n",
    "\n",
    "        \n",
    "        # Denoise data after accumulating a sufficient number of initial samples\n",
    "        if len(accumulated_hor_data) >= initial_samples:\n",
    "            # Limit data history to the most recent 'initial_samples' samples\n",
    "            accumulated_hor_data = accumulated_hor_data[-initial_samples:]\n",
    "            accumulated_ver_data = accumulated_ver_data[-initial_samples:]\n",
    "\n",
    "            # Apply denoising to the accumulated data\n",
    "            denoised_hor_data = denoise_signal(np.array(accumulated_hor_data), wavelet='sym20', level=1)\n",
    "            denoised_ver_data = denoise_signal(np.array(accumulated_ver_data), wavelet='sym20', level=1)\n",
    "\n",
    "            # Queue denoised data for further processing\n",
    "            denoised_hor_queue.put(denoised_hor_data)\n",
    "            denoised_ver_queue.put(denoised_ver_data)\n",
    "            \n",
    "        else:\n",
    "            # Brief pause to mitigate busy waiting\n",
    "            time.sleep(0.001)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9141185f-0f9f-426b-82da-7ca5a539b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_inference_continuous(best_model):\n",
    "    # Set the buffer size for model inference\n",
    "    buffer_size = 20  # Last 20 data points are used\n",
    "    global prev_x_reg, prev_y_reg, blink\n",
    "\n",
    "    # Continuously perform inference using denoised data    \n",
    "    while not exit_event.is_set():\n",
    "        hor_list = denoised_hor_queue.get()\n",
    "        ver_list = denoised_ver_queue.get()\n",
    "        \n",
    "        # Prepare the input for the model\n",
    "        hor_array = np.array(hor_list)\n",
    "        ver_array = np.array(ver_list)\n",
    "        \n",
    "        # Prepare the input for the model\n",
    "        combined_array = np.column_stack((hor_array, ver_array))      # Combine horizontal and vertical data\n",
    "\n",
    "        # Prepare data for model\n",
    "        model_input = torch.tensor(combined_array[-buffer_size:], dtype=torch.float).unsqueeze(0) \n",
    "        model_input = model_input.to(device)\n",
    "        \n",
    "        # Inference with no gradient calculation\n",
    "        with torch.no_grad():\n",
    "            regression_output, classification_output  = best_model(model_input)\n",
    "            regression = (regression_output.cpu()).numpy()  # Convert outputs to CPU NumPy array\n",
    "            blink = (classification_output.cpu()).round()   # Round classification output for blink detection\n",
    "\n",
    "            # Map regression output to pixel space, clamping to screen bounds            \n",
    "            x_clamped = max(min(regression[0][0], 1.0), -1.0)\n",
    "            y_clamped = max(min(regression[0][1], 1.0), -1.0)\n",
    "            x_pixel =  x_clamped * 540\n",
    "            y_pixel = - y_clamped * 960 \n",
    "\n",
    "            # Queue coordinates for visual representation or further processing\n",
    "            circle_coords_queue.put((round(x_pixel), round(y_pixel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09d08f9b-a86f-4eff-bc52-a761c2dce7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # LSTM layer for sequential data processing\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        # Regression output layers for pointer_x and pointer_y\n",
    "        self.regressor = nn.Linear(hidden_dim, 2)\n",
    "        # Classification output layer for blink\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # Get regression values from the last output of LSTM\n",
    "        regress_output = self.regressor(lstm_out[:, -1, :])\n",
    "        # Classify blink with sigmoid activation for probability sigmoid activation\n",
    "        class_output = torch.sigmoid(self.classifier(lstm_out[:, -1, :]))\n",
    "        \n",
    "        return regress_output, class_output.squeeze() # Return coordinates and blink probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4427b6a-68a5-4b7a-b876-b6d7b6d2fbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(2, 64, batch_first=True)\n",
       "  (regressor): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model with the same parameters\n",
    "best_model = LSTMModel(input_dim = 2, hidden_dim = 64, num_layers = 1).to(device)\n",
    "# Load the state dictionary\n",
    "model_path = 'best_lstm_model.pth'  \n",
    "best_model.load_state_dict(torch.load(model_path))\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56421ee1-1a1a-44f2-9891-e34f69dcb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_trend_change(data, data_prev):\n",
    "    # Track current and previous trend directions   \n",
    "    global trend_prev_char, trend_cur_char\n",
    "    trend_cur = data - data_prev # Calculate current trend\n",
    "\n",
    "    # Determine trend direction\n",
    "    if trend_cur > 0: \n",
    "        trend_cur_char = 'positive'\n",
    "    elif trend_cur < 0:\n",
    "        trend_cur_char = 'negative'\n",
    "\n",
    "    # Check for and handle trend change\n",
    "    if trend_cur_char != trend_prev_char:     \n",
    "        trend_prev_char = trend_cur_char # Update previous trend\n",
    "        return data_prev  # Return previous data point if trend changed\n",
    "    else:\n",
    "        return 0    # Return 0 if no trend change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65602061-43a2-424c-91dd-58f997b2c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_circle():\n",
    "    global current_circle_position, circle_item, nearest_center_x_prev, nearest_center_y_prev, x_integrate, y_integrate, label, label_time_start\n",
    "    global flag_timer, focus_timer, activation_flag, activation_label\n",
    "    try:\n",
    "        # Retrieve new position if available\n",
    "        new_x, new_y = circle_coords_queue.get_nowait()\n",
    "    except queue.Empty:\n",
    "        # Schedule another update if no new position data is available\n",
    "        root.after(10, update_circle)\n",
    "        return\n",
    "        \n",
    "     # Calculate the nearest grid center to the new position for smooth movement\n",
    "    nearest_center_x, nearest_center_y = get_nearest_grid_center(new_x, new_y, nearest_center_x_prev, nearest_center_y_prev)\n",
    "\n",
    "    # Detect trend changes to determine circle movement direction\n",
    "    x_trend = detect_trend_change(nearest_center_x, nearest_center_x_prev)\n",
    "    y_trend = detect_trend_change(nearest_center_y, nearest_center_y_prev)\n",
    "\n",
    "    x_integrate += x_trend\n",
    "    y_integrate += y_trend\n",
    "\n",
    "    # Initialize or move the circle based on integrated positions\n",
    "    if current_circle_position is None:\n",
    "        # Create the circle on first update\n",
    "        x_monitor, y_monitor = map_coordinates(x_integrate, y_integrate, mode='monitor')\n",
    "        circle_item = canvas.create_oval(x_monitor - 100, y_monitor - 100, x_monitor + 100, y_monitor + 100, outline=\"blue\", tags=\"circle\")\n",
    "        current_circle_position = (x_integrate, y_integrate)\n",
    "    else:\n",
    "        # Clamping to screen bounds\n",
    "        x_integrate = np.clip(x_integrate, -360, 360)  \n",
    "        y_integrate = np.clip(y_integrate, -760, 760)  \n",
    "        # Calculate the change in position\n",
    "        delta_x = x_integrate - current_circle_position[0]\n",
    "        delta_y = y_integrate - current_circle_position[1]\n",
    "        # Update circle position with clamped values to ensure it stays within bounds\n",
    "        canvas.move(circle_item, delta_x, delta_y)  \n",
    "            \n",
    "        if blink == 1:\n",
    "            # Handle blink detection\n",
    "            blink_message = canvas.create_text(540, 100, text=\"Blink Detected!\", font=(\"Arial\", 30), fill=\"black\")\n",
    "            # Schedule the blink message to be cleared after 1 seconds\n",
    "            root.after(1000, lambda: canvas.delete(blink_message))    \n",
    "            \n",
    "        # Update the current circle position and check for label changes based on user focus    \n",
    "        current_circle_position = (x_integrate, y_integrate)\n",
    "        x_monitor_circle, y_monitor_circle = map_coordinates(x_integrate, y_integrate, mode='monitor')\n",
    "\n",
    "        # Check for changes in the focused label and reset timer if label changes\n",
    "        label_prev = label\n",
    "        label = check_and_print_label_vicinity(x_monitor_circle, y_monitor_circle, label)\n",
    "        if label_prev != label:\n",
    "            label_time_start = time.time()\n",
    "            flag_timer = 1                         # Indicate that the label timer has started\n",
    "            \n",
    "        # If the user focuses on a label for more than 3 seconds, initiate activation sequence    \n",
    "        if (time.time() - label_time_start) >= 3 and label != None and label != \"Main\":\n",
    "            if flag_timer == 1:                       # If focus is still on the same label after 3 seconds\n",
    "                tts.speak(f'Do you want to {label}?') # Ask for confirmation via speech\n",
    "                focus_timer = time.time()             # Reset the focus timer for potential command activation\n",
    "                activation_label = label              # Set the label for potential activation\n",
    "                activation_flag = 1                   # Indicate that the label is ready for activation\n",
    "            else: \n",
    "                if time.time() - focus_timer > 7:   # If no action is taken within 7 seconds\n",
    "                    label_time_start = time.time()  # Reset label timer\n",
    "                    tts.speak(f'time up')           # Inform user that the time for activation has passed\n",
    "                    activation_flag = 0             # Reset activation flag\n",
    "            flag_timer = 0                          # Reset flag timer after processing\n",
    "            \n",
    "        # Activate drone command if the activation flag is set and a blink is detected            \n",
    "        if activation_flag == 1 and blink == 1:\n",
    "            tts.speak(f'activating {activation_label}')  # Announce activation\n",
    "            # Execute drone command based on the activation label\n",
    "            if activation_label == 'Lift':\n",
    "                drone.takeoff()\n",
    "            elif activation_label == 'Land':\n",
    "                drone.land()\n",
    "            elif activation_label == 'Right':\n",
    "                drone.move_right(50)\n",
    "            elif activation_label == 'Left':\n",
    "                drone.move_left(50)     \n",
    "            elif activation_label == 'Forward':\n",
    "                drone.move_forward(50)\n",
    "            elif activation_label == 'Backward':\n",
    "                drone.move_back(50)   \n",
    "            activation_flag = 0  # Reset activation flag after command execution\n",
    "            \n",
    "    # Update previous center coordinates for next iteration        \n",
    "    nearest_center_x_prev = nearest_center_x\n",
    "    nearest_center_y_prev = nearest_center_y\n",
    "\n",
    "    # Schedule the next circle position update\n",
    "    root.after(1, update_circle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2191293c-adc6-480b-a43a-21d2edb9d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_print_label_vicinity(x, y, prev_label):\n",
    "    # Maps labels to their corresponding screen bounds\n",
    "    label_bounds = {\n",
    "        \"Main\": get_label_bounds(\"Main\"),\n",
    "        \"Forward\": get_label_bounds(\"Forward\"),\n",
    "        \"Backward\": get_label_bounds(\"Backward\"),\n",
    "        \"Lift\": get_label_bounds(\"Lift\"),\n",
    "        \"Land\": get_label_bounds(\"Land\"),\n",
    "        \"Left\": get_label_bounds(\"Left\"),\n",
    "        \"Right\": get_label_bounds(\"Right\")\n",
    "    }\n",
    "\n",
    "    # Initialize current_label as None to signify no label is identified initially\n",
    "    current_label = None  \n",
    "\n",
    "    # Iterate over label_bounds to find if (x, y) falls within any label's bounds\n",
    "    for label, bounds in label_bounds.items():\n",
    "        if is_within_bounds(x, y, bounds):\n",
    "            current_label = label            # Update current_label upon finding a match\n",
    "            break                            # Exit loop after the first match to prioritize labels in order\n",
    "    \n",
    "    return current_label  # Return the current label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d775c882-1ddf-4082-bd58-7e9ca31abaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_bounds(x, y, bounds):\n",
    "    # Extract the bounding box coordinates\n",
    "    x1, y1, x2, y2 = bounds\n",
    "    # Check if the point (x, y) lies within the defined bounds\n",
    "    return x1 <= x <= x2 and y1 <= y <= y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "205f8b3f-dd94-48e5-bf66-f78e967ea1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_coordinates(x, y, mode='cartesian'):\n",
    "    \"\"\"\n",
    "    Translates coordinates between Cartesian and monitor spaces. This is useful for applications\n",
    "    that require the conversion of geometric positions to screen positions and vice versa.\n",
    "\n",
    "    Parameters:\n",
    "    - x, y: The coordinates to map.\n",
    "    - mode: Specifies the direction of mapping. 'cartesian' converts monitor to Cartesian coordinates,\n",
    "            while 'monitor' converts Cartesian to monitor coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - The mapped coordinates (x_mapped, y_mapped) in the specified space.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert monitor space to Cartesian space\n",
    "    if mode == 'cartesian':\n",
    "        # Map from monitor space to Cartesian space\n",
    "        x_mapped = x - 540  # Shift x to center at 0 (assuming a screen width of 1080px)\n",
    "        y_mapped = y - 960  # Shift and invert y to center at 0 (assuming a screen height of 1920px)       \n",
    "    # Convert Cartesian space to monitor space    \n",
    "    elif mode == 'monitor':\n",
    "        x_mapped = x + 540  # Shift x to start from the left edge of the screen\n",
    "        y_mapped = 960 + y  # Invert and shift y to start from the top edge of the screen\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified. Use 'cartesian' or 'monitor'.\")\n",
    "    \n",
    "    return x_mapped, y_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f7aa10-856c-4128-a1aa-3274e1573c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_grid_center(x, y, prev_center_x = None, prev_center_y = None, border_tolerance_ratio = 0.1):\n",
    "    # Map input coordinates to monitor space and define grid parameters\n",
    "    x_monitor, y_monitor = map_coordinates(x, y, mode = 'monitor')\n",
    "    screen_width = 1080\n",
    "    screen_height = 1920\n",
    "    desired_grid_size = 360\n",
    "    \n",
    "    # Calculate grid dimensions and cell size\n",
    "    grid_cols = screen_width // desired_grid_size\n",
    "    grid_rows = screen_height // desired_grid_size\n",
    "    cell_width = screen_width / grid_cols\n",
    "    cell_height = screen_height / grid_rows\n",
    "\n",
    "    # Identify grid cell of the current point\n",
    "    col = int(x_monitor // cell_width)\n",
    "    row = int(y_monitor // cell_height)\n",
    "    center_x = (col + 0.5) * cell_width\n",
    "    center_y = (row + 0.5) * cell_height\n",
    "\n",
    "    # Convert the calculated center back to Cartesian coordinates\n",
    "    x_cart, y_cart = map_coordinates(center_x, center_y, mode = 'cartesian')\n",
    "\n",
    "    # Compare with previous center if provided, to determine if snapping back is warranted\n",
    "    if prev_center_x is not None and prev_center_y is not None:\n",
    "        # Convert previous centers to monitor coordinates for comparison\n",
    "        prev_center_x_monitor, prev_center_y_monitor = map_coordinates(prev_center_x, prev_center_y, mode = 'monitor')\n",
    "\n",
    "        # Calculate distances to previous and new centers\n",
    "        dist_to_prev_center = ((x_monitor - prev_center_x_monitor) ** 2 + (y_monitor - prev_center_y_monitor) ** 2) ** 0.5\n",
    "        dist_to_new_center = ((x_monitor - center_x) ** 2 + (y_monitor - center_y) ** 2) ** 0.5\n",
    "        \n",
    "        # Calculate border tolerance based on cell size\n",
    "        border_tolerance = min(cell_width, cell_height) * border_tolerance_ratio\n",
    "\n",
    "        # Decide whether to return the new center or stick with the previous one\n",
    "        if dist_to_prev_center < border_tolerance and (dist_to_new_center == 0 or (dist_to_new_center - dist_to_prev_center) / dist_to_new_center < 0.3):\n",
    "            return prev_center_x, prev_center_y\n",
    "\n",
    "    return x_cart, y_cart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01300a67-8ca5-46de-8bc5-989b73ec6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_bounds(label_name):\n",
    "    \"\"\"\n",
    "    Determines the bounding box for a given label on the GUI. These boxes are used to map user gaze\n",
    "    to specific commands. The function defines the size and position of each label's bounding box based on\n",
    "    the label's name.\n",
    "\n",
    "    Parameters:\n",
    "    - label_name: The name of the label for which to get the bounds.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple representing the bounding box (x1, y1, x2, y2) of the label, or None if the label is not recognized.\n",
    "    \"\"\"\n",
    "    # Define dimensions and spacing for the bounding boxes\n",
    "    # Dimensions of the label boxes\n",
    "    box_width = 200 \n",
    "    box_height = 50 \n",
    "    # Spacing between boxes\n",
    "    vertical_space = 400\n",
    "    horizontal_space = 400\n",
    "    # Screen center coordinates\n",
    "    center_x = 540\n",
    "    center_y = 960\n",
    "    \n",
    "    if label_name == \"Main\":\n",
    "        return (center_x - box_width // 2, center_y - box_height // 2, center_x + box_width // 2, center_y + box_height // 2)\n",
    "    elif label_name == \"Forward\":\n",
    "        return (center_x - box_width // 2, center_y - vertical_space - box_height // 2, center_x + box_width // 2, center_y - vertical_space + box_height // 2)\n",
    "    elif label_name == \"Backward\":\n",
    "        return (center_x - box_width // 2, center_y + vertical_space - box_height // 2, center_x + box_width // 2, center_y + vertical_space + box_height // 2)\n",
    "    elif label_name == \"Right\":\n",
    "        return (center_x + horizontal_space - box_width // 2, center_y - box_height // 2, center_x + horizontal_space + box_width // 2, center_y + box_height // 2)\n",
    "    elif label_name == \"Left\":\n",
    "        return (center_x - horizontal_space - box_width // 2, center_y - box_height // 2, center_x - horizontal_space + box_width // 2, center_y + box_height // 2)\n",
    "    elif label_name == \"Lift\":\n",
    "        return (center_x + horizontal_space - box_width // 2, center_y - vertical_space - box_height // 2, center_x + horizontal_space + box_width // 2, center_y - vertical_space + box_height // 2)\n",
    "    elif label_name == \"Land\":\n",
    "        return (center_x - horizontal_space - box_width // 2, center_y + vertical_space - box_height // 2, center_x - horizontal_space + box_width // 2, center_y + vertical_space + box_height // 2)\n",
    "    else:\n",
    "        return None  # Return None if label name doesn't match predefined options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68ad1abe-e111-464a-802c-8913dd9fac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to gracefully exit the application.\n",
    "def quit_app(event = None):\n",
    "    exit_event.set() # Signal all threads or processes to terminate\n",
    "    root.destroy()   # Close the GUI window, effectively ending the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fad359e4-6893-4722-ac1c-85d5976a3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines a frame and a label into a single element.\n",
    "def create_labeled_frame(master, text, bg_color, fg_color, border_color, border_width, x, y):\n",
    "    # Create and place the frame at specified coordinates\n",
    "    frame = tk.Frame(master, width=box_width, height = box_height, bg = bg_color, highlightbackground = border_color, highlightthickness = border_width)\n",
    "    frame.place(x = x, y = y)\n",
    "    frame.pack_propagate(False)  # Prevents the frame from resizing to fit the label\n",
    "    # Create and pack the label inside the frame\n",
    "    label = tk.Label(frame, text = text, bg = bg_color, fg = fg_color, font = (\"Arial\", font_size))\n",
    "    label.pack(expand = True, fill = 'both')\n",
    "    return frame    # Return the frame containing the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb6a1a53-a983-43cc-8d30-5c04232c1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_threads():\n",
    "    \"\"\"Function to start all threads.\"\"\"\n",
    "    serial_thread.start()     # Begins serial data collection\n",
    "    denoise_thread.start()    # Starts signal denoising process\n",
    "    inference_thread.start()  # Initiates neural network inference operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2e00d8c-5877-4d94-a241-aacdd6824f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncTTS:\n",
    "    \"\"\"\n",
    "    A class for asynchronous text-to-speech operations, enabling non-blocking speech synthesis.\n",
    "    Utilizes a queue to manage speech requests and a separate thread for processing these requests.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize the speech synthesis engine\n",
    "        self.engine = pyttsx3.init()\n",
    "        # Queue for holding text-to-speech requests\n",
    "        self.speech_queue = queue.Queue()\n",
    "        # Flag to indicate if the queue is currently being processed\n",
    "        self.is_running = False\n",
    "\n",
    "    def speak(self, text):\n",
    "        \"\"\"\n",
    "        Adds text to the speech queue and starts processing the queue if not already doing so.\n",
    "        \"\"\"\n",
    "        self.speech_queue.put(text)         # Add text to the queue\n",
    "        if not self.is_running:\n",
    "            # Start a new thread to process the queue if one isn't already running\n",
    "            threading.Thread(target=self._process_queue).start()\n",
    "\n",
    "    def _process_queue(self):\n",
    "        \"\"\"\n",
    "        Processes the speech queue, synthesizing each text request in sequence.\n",
    "        Runs on a separate thread to avoid blocking the main application.\n",
    "        \"\"\"\n",
    "        self.is_running = True                   # Indicate processing has started\n",
    "        while not self.speech_queue.empty():\n",
    "            text = self.speech_queue.get()       # Retrieve the next text to speak\n",
    "            self.engine.say(text)                # Pass the text to the speech engine\n",
    "            self.engine.runAndWait()             # Wait for the speech synthesis to complete\n",
    "        self.is_running = False                  # Indicate processing is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "452a525a-6ca0-4f9f-9668-6c5062e3d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of the AsyncTTS class for asynchronous text-to-speech operations.\n",
    "tts = AsyncTTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33504831-6c4b-4c84-a7b8-7232aec675fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] tello.py - 438 - Send command: 'takeoff'\n",
      "[INFO] tello.py - 462 - Response takeoff: 'ok'\n",
      "[INFO] tello.py - 438 - Send command: 'left 50'\n",
      "[INFO] tello.py - 462 - Response left 50: 'ok'\n"
     ]
    }
   ],
   "source": [
    "# Entry point of the application when run \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize global variables and setup the GUI\n",
    "        current_circle_position, circle_item = None, None\n",
    "        prev_x_reg, prev_y_reg = 540, 960                    # Default starting position\n",
    "        trend_prev_char, trend_cur_char = None, None         # Trend tracking\n",
    "        nearest_center_x_prev, nearest_center_y_prev = 0, 0  # Previous grid center    \n",
    "        x_integrate, y_integrate = 0, 0                      # Integrated position for smooth movement\n",
    "        label = None\n",
    "        flag_timer, activation_flag = 0, 0                   # Timers and flags for label activation\n",
    "        \n",
    "        # Initialize serial connection for data acquisition for Linux\n",
    "        ser = serial.Serial('/dev/ttyACM0',9600)\n",
    "        \n",
    "        # Setup queues for thread communication and event for thread termination\n",
    "        raw_data_queue = Queue()\n",
    "        denoised_hor_queue = Queue()\n",
    "        denoised_ver_queue = Queue()\n",
    "        circle_coords_queue = Queue()\n",
    "        exit_event = threading.Event()\n",
    "\n",
    "        # Setup the main GUI window\n",
    "        root = tk.Tk()\n",
    "        root.title(\"Control Panel\")\n",
    "        root.attributes('-fullscreen', True)  # Fullscreen mode\n",
    "        root.bind('<Escape>', quit_app)       # Escape key binds to quit\n",
    "         \n",
    "        # Calculate center positions\n",
    "        screen_width = root.winfo_screenwidth()        \n",
    "        screen_height = root.winfo_screenheight()\n",
    "        center_x = screen_width // 2\n",
    "        center_y = screen_height // 2\n",
    "\n",
    "        # Create the main canvas\n",
    "        canvas = tk.Canvas(root, width=root.winfo_screenwidth(), height=root.winfo_screenheight())\n",
    "        canvas.pack()\n",
    "        \n",
    "        # Customize the size of buttons and labels\n",
    "        box_width = 200 # Increase box width\n",
    "        box_height = 50 # Increase box height\n",
    "        font_size = \"30\"  # Increase font size\n",
    "        \n",
    "        # Create frames with labels using the function\n",
    "        vertical_space = 400\n",
    "        horizontal_space = 400\n",
    "        main_frame = create_labeled_frame(root, \"Main\", \"orange\", \"black\", \"red\", 5, center_x - box_width // 2, center_y-box_height//2)\n",
    "        forward_frame = create_labeled_frame(root, \"Forward\", \"orange\", \"black\", \"red\", 5, center_x - box_width // 2, center_y - vertical_space - box_height // 2)\n",
    "        backward_frame = create_labeled_frame(root, \"Backward\", \"orange\", \"black\", \"red\", 5, center_x - box_width // 2, center_y + vertical_space - box_height // 2)\n",
    "        right_frame = create_labeled_frame(root, \"Right\", \"orange\", \"black\", \"red\", 5, center_x + horizontal_space - box_width // 2, center_y - box_height // 2)\n",
    "        left_frame = create_labeled_frame(root, \"Left\", \"orange\", \"black\", \"red\", 5, center_x - horizontal_space - box_width // 2, center_y - box_height // 2)\n",
    "        lift_frame = create_labeled_frame(root, \"Lift\", \"orange\", \"black\", \"red\", 5, center_x + horizontal_space - box_width // 2, center_y - vertical_space - box_height // 2)\n",
    "        land_frame = create_labeled_frame(root, \"Land\", \"orange\", \"black\", \"red\", 5, center_x - horizontal_space - box_width // 2, center_y + vertical_space - box_height // 2)\n",
    "        \n",
    "        # Quit button\n",
    "        quit_button = tk.Button(root, text=\"Quit\", command=quit_app)\n",
    "        quit_button.place(x=0, y=0)  # Position the quit button at the top left corner\n",
    "        \n",
    "        # Start threads for handling serial data, denoising, and inference\n",
    "        serial_thread = threading.Thread(target=read_serial_continuous)\n",
    "        denoise_thread = threading.Thread(target=denoise_signal_continuous)\n",
    "        inference_thread = threading.Thread(target=nn_inference_continuous, args=(best_model,))\n",
    "       \n",
    "        # Schedule the first tasks and start the GUI event loop\n",
    "        root.after(1, start_threads)        # Delay thread start to ensure GUI initializes properly\n",
    "        root.after(100, update_circle)      # Begin updating the circle's position\n",
    "    \n",
    "        root.mainloop()                     # Start the GUI\n",
    "\n",
    "    finally:\n",
    "        # Cleanup on exit\n",
    "        ser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6459f-8fee-4477-bcc7-2c28049cb618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819cc508-2a04-464b-887e-c901208206d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    root.destroy()  # Close the GUI window If not close automatically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
