# GazeAviator
![image](https://github.com/Nil-Del/GazeAviator/assets/59746481/c1a4b451-9722-4917-8073-265bc882e09e)
## Eye Tracking:
In eye tracking, Video Oculography (VOG) and Electrooculography (EOG) are two notable methods. VOG tracks eye movements with video technology, requiring good lighting for optimal performance. However, VOG equipment tends to be costly. EOG, in contrast, detects the subtle electric signals generated by eye movements. It's not only cost-effective but also versatile, functioning well in any lighting condition. EOG's cost-effectiveness and versatility in any lighting make it the ideal choice for our project's foundation.

EOG operates on the principle of the electrical potential difference between the cornea, which acts as the positive pole, and the retina, the negative pole. This creates a corneoretinal potential, turning the eye into an effective electric dipole (Fig. 1). 
![image](https://github.com/Nil-Del/GazeAviator/assets/59746481/a1090340-d8a4-416f-8e16-e381ace97119)

As the eye shifts position, this dipole's orientation changes, altering the potential difference detectable at the skin's surface with strategically placed electrodes. For instance, moving the eyes left increases the potential at the left eye's outer corner and decreases it at the right's, leading to a negative potential difference. Conversely, looking right reverses this effect, creating a positive potential difference. When the eyes are stationary and facing forward, the potential measured by the electrodes fluctuate around zero, providing a baseline for eye movement detection (Fig. 2).
![image](https://github.com/Nil-Del/GazeAviator/assets/59746481/a8a0040a-8f88-4818-81ab-28507e4db59b)

## Hardware Setup
In this project, silver (Ag) electrodes are used to measure EOG signals. To measure vertical eye movements, electrodes are attached to above and below of one eye. A reference electrode is affixed behind the ear To capture the horizontal movements, two electrodes are placed approximately 1cm from the outer canthus of each eye.

The signals from the electrodes are then channeled into a "BioAmp EXG Pill" analog-front-end (AFE) biopotential signal-acquisition board. The board's purpose is to amplify the raw EOG signals to a level suitable for effective digitization and processing. Given the weak nature of the signals emitted by the human eye, such amplification is a vital step in securing accurate and reliable data acquisition.

This module incorporates a Driven Right Leg (DRL) circuit to mitigate common-mode noise, which can result from a variety of sources. The DRL circuit functions by first identifying the common-mode voltage present in the EOG signal inputs to the differential amplifier. It then inverts this voltage and reintroduces it to the body through the "right leg" electrode. This action effectively diminishes the common-mode voltage that the amplifier detects, thereby reducing the impact of noise on the EOG signal. Additionally, the board is equipped with an amplifier and a bandpass filter with an inherent differentiator which makes the analog output signal (0 to 5v) appear in the form of spikes. (Soldering iron, soldering wire are required to solder the header pins to the module.)

Following signal amplification and filtering, the analog signal needs to be digitized for processing within the computer. This is achieved by first passing the analog signal to the Analog-to-Digital Converter (ADC) port of the Arduino Uno (vertical and horizontal EOG connected to Pin A0 and A1). This ADC has a resolution of 10 bits, meaning the analog signal, which ranges from 0-5 volts, is scaled to a digital value ranging from 0-1023.

Once digitized, the signal is then transmitted to the NVIDIA Jetson Orin Nano development kit via USB serial communication at a baud rate of 9600 bits/sec. NVIDIA Jetson Orin Nano development kit connects to the Tello DJI drone over a Wi-Fi connection, providing real-time control based on the user's eye movements. The Full Schematic is shown in Fig. 3.

![image](https://github.com/Nil-Del/GazeAviator/assets/59746481/95886f78-1c1c-4022-8665-0b854189c009)

## AI Pipeline
The goal of this project is to enable eye-controlled drone navigation via a graphical user interface (GUI). We envisioned a GUI equipped with buttons that could interpret the user's gaze shifting across the screen, visually indicating gaze location with a circle. More specifically, when a user's gaze remains fixed on a button and is followed by a blink, the GUI sends a corresponding command to the drone.

Taking the sequential nature of EOG data into account guides us to employ a Long Short-Term Memory (LSTM) model. We train this model to convert sequences of EOG signals into screen coordinates, effectively mapping eye movements to specific pixels on the monitor. The first essential step in this approach is collecting dataset of EOG signals for the LSTM model's training. Once the dataset of EOG signals is collected, the next step is training the LSTM model with this data. After training, the model is ready for deployment in our application, where it interprets eye movements in real-time to control the GUI, seamlessly translating gaze into precise commands for interaction.
![image](https://github.com/Nil-Del/GazeAviator/assets/59746481/aba18449-4d7b-4264-ba2d-4a17dc165c13)

## Demo

### Dataset Collection
https://youtu.be/2nfEBO3IliI

### UAV navigation using eye gaze
https://youtu.be/hH0xG9Rlz_4



